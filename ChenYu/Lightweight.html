<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Welcome to EMRG</title><meta name="author" content="Yu Zhou"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/css/fontawesome.all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/"></a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/"> Home</a></li><li class="menus_item"><a class="site-page" href="/projects.html"> Projects</a></li><li class="menus_item"><a class="site-page" href="/students.html"> Members</a></li><li class="memus_item"><a class="site-page" href="/zh-cn/ChenYu/Lightweight.html"> 简体中文</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/assets/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Yu Zhou</h3><p class="author-bio">Research Institute for Future Media Computing, Shenzhen University.</p></div><div class="author-links"><ul class="social-links"><li><a class="e-social-link" id="_email" href="" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i><span>Email</span><script>document.getElementById('_email').href = atob('bWFpbHRvOnl1Lnpob3VAc3p1LmVkdS5jbg==')</script></a></li><li><a class="e-social-link" href="https://scholar.google.com/citations?user=7i308qgAAAAJ" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://orcid.org/0000-0001-5760-4102" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Project</h2><article><h3 id="A-Lightweight-Recurrent-Learning-Network-for-Sustainable-Compressed-Sensing"><a href="#A-Lightweight-Recurrent-Learning-Network-for-Sustainable-Compressed-Sensing" class="headerlink" title="A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing"></a>A Lightweight Recurrent Learning Network for Sustainable Compressed Sensing</h3><ul>
<li>Author: Yu Zhou, Yu Chen, Pan Lai, Lei Huang, Jianmin Jiang </li>
<li>Accepted:  20 March 2023 </li>
<li>Published in: IEEE Transactions on Emerging Topics in Computational Intelligence ( TETCI )</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10124723">paper link</a></li>
<li>code：<a target="_blank" rel="noopener" href="https://github.com/C66YU/CSRN">https://github.com/C66YU/CSRN</a></li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>Recently, deep learning-based compressed sensing (CS) has achieved great success in reducing the sampling and computational cost of sensing systems and improving the reconstruction quality. These approaches, however, largely overlook the issue of the computational cost; they rely on complex structures and task specific operator designs, resulting in extensive storage and high energy consumption in CS imaging systems. In this article, we propose a lightweight but effective deep neural network based on recurrent learning to achieve a sustainable CS system; it requires a smaller number of parameters but obtains high-quality reconstructions. Specifically, our proposed network consists of an initial reconstruction sub-network and a residual reconstruction sub-network.While the initial reconstruction sub-network has a hierarchical structure to progressively recover the image, reducing the number of parameters, the residual reconstruction sub-network facilitates recurrent residual feature extraction via recurrent learning to perform both feature fusion and deep reconstructions across different scales. In addition, we also demonstrate that, after the initial reconstruction, feature maps with reduced sizes are sufficient to recover the residual information, and thus we achieved a significant reduction in the amount of memory required. Extensive experiments illustrate that our proposed model can achieve a better reconstruction quality than existing state-of-the-art CS algorithms, and it also has a smaller number of network parameters than these algorithms. Our source codes are available at: <a target="_blank" rel="noopener" href="https://github.com/C66YU/CSRN">https://github.com/C66YU/CSRN</a>.</p>
<p><img src="/../assets/PicForS/picforCY_1.png" alt="框架流程图"></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> Home</a></li><li class="nav_item"><a class="nav-page" href="/projects.html"> Projects</a></li><li class="nav_item"><a class="nav-page" href="/students.html"> Students</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2023 by Yu Zhou</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="/js/jquery.min.js"></script><script src="/js/main.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>