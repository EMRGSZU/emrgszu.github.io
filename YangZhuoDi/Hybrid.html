<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Welcome to EMRG</title><meta name="author" content="Yu Zhou"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/css/fontawesome.all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/"></a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/"> Home</a></li><li class="menus_item"><a class="site-page" href="/projects.html"> Projects</a></li><li class="menus_item"><a class="site-page" href="/students.html"> Members</a></li><li class="memus_item"><a class="site-page" href="/zh-cn/YangZhuoDi/Hybrid.html"> 简体中文</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/assets/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Yu Zhou</h3><p class="author-bio">Research Institute for Future Media Computing, Shenzhen University.</p></div><div class="author-links"><ul class="social-links"><li><a class="e-social-link" id="_email" href="" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i><span>Email</span><script>document.getElementById('_email').href = atob('bWFpbHRvOnl1Lnpob3VAc3p1LmVkdS5jbg==')</script></a></li><li><a class="e-social-link" href="https://scholar.google.com/citations?user=7i308qgAAAAJ" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://orcid.org/0000-0001-5760-4102" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Project</h2><article><h3 id="A-Hybrid-Attention-Based-Deep-Neural-Network-for-Simultaneous-Multi-Sensor-Pruning-and-Human-Activity-Recognition"><a href="#A-Hybrid-Attention-Based-Deep-Neural-Network-for-Simultaneous-Multi-Sensor-Pruning-and-Human-Activity-Recognition" class="headerlink" title="A Hybrid Attention-Based Deep Neural Network for Simultaneous Multi-Sensor Pruning and Human Activity Recognition"></a>A Hybrid Attention-Based Deep Neural Network for Simultaneous Multi-Sensor Pruning and Human Activity Recognition</h3><ul>
<li>Author: Yu Zhou, Zhuodi Yang, Xiao Zhang, Yufan Wang</li>
<li>Accepted: 25 July 2022</li>
<li>Published in:  IEEE Internet of Things Journal( IEEE INTERNET THINGS )</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9848473">Paper Link</a></li>
<li>code：<a target="_blank" rel="noopener" href="https://github.com/EMRGSZU/papers-code">https://github.com/EMRGSZU/papers-code</a></li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>With the popularity and development of Internet of Things (IoT) technology, human activity recognition using IoT devices such as wearable sensors can be implemented for various applications. Due to the complexity of activity recognition, multiple homogeneous or heterogeneous sensors are used to obtain excessive information in most wearable activity recognition systems. However, the increased number of sensors and the way of multichannel signal data bring huge challenges to human activity recognition tasks. How to select suitable sensor channels to balance the computational complexity and recognition accuracy has become a major issue. In this article, we extend the sparse group Lasso mechanism to human activity recognition tasks and propose a hybrid attention-based multi-sensor pruning and feature selection deep neural network, called HAP-DNN. This architecture is able to further perform feature selection on the basis of sensor pruning. HAP-DNN consists of three detachable modules: 1) a feature compression &amp; reconstruction module for sensor feature information fusion and restoration; 2) a feature weight calculation module for calculating sensor channel weights and feature weights; and 3) a learning module for classification, which can be regarded as a filter feature selection method. Four public activity recognition data sets are used to verify our proposed architecture, and the experimental results show that HAP-DNN achieves the best classification performance with the least number of retained feature channels.</p>
<p><img src="/../../assets/PicForS/picforYZD_1.png" alt="框架流程图"></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> Home</a></li><li class="nav_item"><a class="nav-page" href="/projects.html"> Projects</a></li><li class="nav_item"><a class="nav-page" href="/students.html"> Students</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2023 by Yu Zhou</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="/js/jquery.min.js"></script><script src="/js/main.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>