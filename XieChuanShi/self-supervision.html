<!DOCTYPE html><html><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Welcome to EMRG</title><meta name="author" content="Yu Zhou"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="/css/fontawesome.all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/"></a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/"> Home</a></li><li class="menus_item"><a class="site-page" href="/projects.html"> Projects</a></li><li class="menus_item"><a class="site-page" href="/students.html"> Members</a></li><li class="memus_item"><a class="site-page" href="/zh-cn/XieChuanShi/self-supervision.html" onclick="localStorage.setItem(&quot;language&quot;, &quot;zh-cn&quot;)"> 简体中文</a></li></ul></div><script>var languagesUrlMap = {"zh-cn": "/zh-cn/XieChuanShi/self-supervision.html","en": "/XieChuanShi/self-supervision.html",}</script><script>let userLang = localStorage.getItem('language');
if (!userLang) {
    const navigatorLang = navigator.language || navigator.userLanguage;
    userLang = navigatorLang.substr(0, 2) === "zh" ? "zh-cn" : "en";
}
const currentLang = location.href.includes("zh-cn") ? "zh-cn" : "en";
if (userLang !== currentLang) {
    location.href = languagesUrlMap[userLang];
}</script></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/assets/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3><a href="/zhouyu.html">Yu Zhou</a></h3><p class="author-bio">Research Institute for Future Media Computing, Shenzhen University.</p></div><div class="author-links"><ul class="social-links"><li><a class="e-social-link" id="_email" href="" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i><span>Email</span><script>document.getElementById('_email').href = atob('bWFpbHRvOnl1Lnpob3VAc3p1LmVkdS5jbg==')</script></a></li><li><a class="e-social-link" href="https://scholar.google.com/citations?user=7i308qgAAAAJ" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://orcid.org/my-orcid?orcid=0000-0002-3224-0063" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Project</h2><article><h3 id="A-Self-Supervised-Human-Activity-Recognition-Approach-via-Body-Sensor-Networks-in-Smart-City"><a href="#A-Self-Supervised-Human-Activity-Recognition-Approach-via-Body-Sensor-Networks-in-Smart-City" class="headerlink" title="A Self-Supervised Human Activity Recognition Approach via Body Sensor Networks in Smart City"></a>A Self-Supervised Human Activity Recognition Approach via Body Sensor Networks in Smart City</h3><ul>
<li>Author: Yu Zhou,  Chuanshi Xie, Shilong Sun, Xiao Zhang, Yufan Wang</li>
<li>Accepted: 8 June 2023</li>
<li>Published in:  IEEE Sensors Journal( JSEN )</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/10147042">Paper Link</a></li>
<li>code：<a target="_blank" rel="noopener" href="https://github.com/EMRGSZU/papers-code">https://github.com/EMRGSZU/papers-code</a></li>
</ul>
<h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>In smart cities, pervasive sensing and wearable computing techniques are increasingly being employed to monitor and recognize human activities through body sensor networks, which have been widely used in urban safety, healthcare, and manufacturing. However, most researchers regard human activity recognition (HAR) as a high-cost research task requiring a large amount of labeled data, which is often unrealistic in real-world applications. To address this problem, we propose a self-supervised learning framework for HAR (SS-HAR). SS-HAR initially takes the unlabeled data generated by data augmentation as the input of the network, mines the supervised information of the unlabeled data under the effect of self-supervised learning, and uses the obtained backbone network as a feature extractor to extract activity features for subsequent classification. After that, we use part of the labeled data as the training set and extract the activity features using the backbone network for training and fitting the classifier. Then we utilize the rest of the data to verify the feasibility and effectiveness of the proposed self-supervised learning method. We have conducted multiple experiments on three publicly available datasets and one self-collected basketball activity dataset SZU_HAD_Basketball. The experimental results show that the SS-HAR method is able to achieve higher classification accuracy and stability than supervised and semi-supervised methods. Specifically, on the UCI dataset, SS-HAR achieves better classification performance compared to other approaches which improve the classification accuracy by 1% over the supervised method and by 5%-6% over the semi-supervised method respectively.</p>
<p><img src="/../../assets/PicForS/picforXCS.png" alt="框架流程图"></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/"> Home</a></li><li class="nav_item"><a class="nav-page" href="/projects.html"> Projects</a></li><li class="nav_item"><a class="nav-page" href="/students.html"> Students</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by Yu Zhou</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="/js/jquery.min.js"></script><script src="/js/main.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></body></html>